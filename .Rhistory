apply(badData, 2, sumstat_fun)
apply(goodData, 2, sumstat_fun)
allData = subset(allData, select = -c("X", "id", "uri", "track_href", "analysis_url") )
names(allData)
drop <- c("X", "id", "uri", "track_href", "analysis_url")
allData = allData[,!(names(allData) %in% drop)]
names(allData)
View(allData)
# read in data, got from python using Spotipi with Spotify Web API
allData <- read.csv("~/Desktop/Spotify/featuresData.csv")
badData <- read.csv("~/Desktop/Spotify/badFeatures.csv")
goodData <- read.csv("~/Desktop/Spotify/goodFeatures.csv")
names(allData)
drop <- c("X", "id", "uri", "track_href", "analysis_url")
allData = allData[,!(names(allData) %in% drop)]
badData = badData[,!(names(badData) %in% drop)]
goodData = goodData[,!(names(goodData) %in% drop)]
names(allData)
#Initial Summary of data
sumstat_fun<- function(x){
c(Mean=mean(x, na.rm=T), Std=sd(x, na.rm = T),
Min=min(x, na.rm = T), Max=max(x, na.rm = T))
}
apply(badData, 2, sumstat_fun)
apply(goodData, 2, sumstat_fun)
# read in data, got from python using Spotipi with Spotify Web API
allData <- read.csv("~/Desktop/Spotify/featuresData.csv")
badData <- read.csv("~/Desktop/Spotify/badFeatures.csv")
goodData <- read.csv("~/Desktop/Spotify/goodFeatures.csv")
names(allData)
drop <- c("X", "id", "uri", "track_href", "analysis_url", "type")
allData = allData[,!(names(allData) %in% drop)]
badData = badData[,!(names(badData) %in% drop)]
goodData = goodData[,!(names(goodData) %in% drop)]
names(allData)
#Initial Summary of data
sumstat_fun<- function(x){
c(Mean=mean(x, na.rm=T), Std=sd(x, na.rm = T),
Min=min(x, na.rm = T), Max=max(x, na.rm = T))
}
apply(badData, 2, sumstat_fun)
apply(goodData, 2, sumstat_fun)
ggplot(data=allData, aes(x=apply(allData, 2, sumstat_fun), y=len, fill=Like)) +
geom_bar(stat="identity")
library(ggplot2)
ggplot(data=allData, aes(x=apply(allData, 2, sumstat_fun), y=len, fill=Like)) +
geom_bar(stat="identity")
library(ggplot2)
ggplot(data=allData, aes(x=apply(allData, 2, sumstat_fun), fill=Like)) +
geom_bar(stat="identity")
library(ggplot2)
ggplot(data=allData, aes(x="Category", y=ap1 , fill=Like)) +
geom_bar(stat="identity")
ap1 <-apply(badData, 2, sumstat_fun)
ap2 <- apply(goodData, 2, sumstat_fun)
ggplot(data=allData, aes(x="Category", y=ap1 , fill=Like)) +
geom_bar(stat="identity")
plot(badData$danceability)
plot(badData$energy)
library(ggplot2)
plot(badData$danceability)
plot(badData$energy)
ggplot(allData, aes(x=danceability, color=Like))+
geom_point()
ggplot(allData, aes(y=danceability, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=danceability, color=Like))+
geom_point()
names(allData)
library(ggplot2)
plot(badData$danceability)
plot(badData$energy)
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=danceability, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=energy, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=key, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=loudness, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=mode, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=speechiness, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=acousticness, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=instrumentalness, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=liveness, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=valence, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=tempo, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=duration_ms, color=Like))+
geom_point()
ggplot(allData, aes(x = as.numeric(row.names(allData)), y=time_signature, color=Like))+
geom_point()
index<- sample(1:nrow(allData), 0.8*nrow(allData))
train <- allData[index,]
test <- allData[-index,]
model1<- glm(Like~., data=train, family=binomial)
summary(model1)
library(rpart)
library(rpart.plot)
tree1 <- rpart(formula = Like~., data=train, method="class")
prp(tree1, digits = 4, extra = 1)
library(ROCR)
## training sample
pred.model1.train<- predict(model1, type="response")
pred <- prediction(pred.model1.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
plot(perf, colorize=TRUE)
summary(model1)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms, data=train, family=binomial)
summary(model2)
model1<- glm(Like~., data=train, family=binomial, type="response")
summary(model1)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instramentalness, data=train, family=binomial)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness, data=train, family=binomial)
summary(model2)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo, data=train, family=binomial)
summary(model2)
pred.model2.train<- predict(model2, type="response")
pred <- prediction(pred.model2.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
pred.model1.train<- predict(model1, type="response")
pred <- prediction(pred.model1.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo, data=train, family=binomial)
summary(model2)
pred.model2.train<- predict(model2, type="response")
pred <- prediction(pred.model2.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
library(rpart)
library(rpart.plot)
tree1 <- rpart(formula = Like~., data=train, method="class")
prp(tree1, digits = 4, extra = 1)
test.pred.tree1 <- predict(tree1, newdata=test, type="class")
# Confusion Matrix
table(test$Like, test.pred.tree1, dnn = c("True", "Pred"))
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree1, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pred <- prediction(pred.prob.model2, bankruptcy.test$DLRSN)
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pred <- prediction(pred.prob.model2, test$Like)
confMatr <- table(test$Like, pred.prob.model2, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pcut<- mean(allData$Like)
class.model2<- (pred.prob.model2>pcut)*1
confMatr <- table(test$Like, class.model2, dnn = c("True", "Predicted"))
prop.table(confMatr, 1)
tree.large <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(tree.large, digits = 4, extra = 1)
plotcp(tree.large)
tree2 <- rpart(formula = Like~., data=train, method="class", cp=0.017)
prp(tree2, digits = 4, extra = 1)
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree2, dnn = c("True", "Pred"))
test.pred.tree2 <- predict(tree2, newdata=train, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree2, dnn = c("True", "Pred"))
tree2 <- rpart(formula = Like~., data=train, method="class", cp=0.017)
prp(tree2, digits = 4, extra = 1)
test.pred.tree2 <- predict(tree2, newdata=test, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree2, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
# Misclassification Rate -- (FP+FN)/Total
MR2 <- mean(test$Like!=test.pred.tree2)
MR2
# AUC
tree2.test.prob.rpart<- predict(tree2,bankruptcy.test, type="prob")
# AUC
tree2.test.prob.rpart<- predict(tree2,test, type="prob")
pred = prediction(tree2.test.prob.rpart[,2], test$Like)
AUC2 <- slot(performance(pred, "auc"), "y.values")[[1]]
AUC2
pred.model1.train<- predict(model1, type="response")
pred <- prediction(pred.model1.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
a
pcut<- mean(allData$Like)
class.model2<- (pred.prob.model2>pcut)*1
confMatr <- table(test$Like, class.model2, dnn = c("True", "Predicted"))
prop.table(confMatr, 1)
mean(test$Like!=pred.prob.model2)
mean(test$Like!=class.model2)
MR2
library(rpart)
library(rpart.plot)
tree1 <- rpart(formula = Like~., data=train, method="class")
prp(tree1, digits = 4, extra = 1)
test.pred.tree1 <- predict(tree1, newdata=test, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree1, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
mean(test$Like!=test.pred.tree1)
index<- sample(1:nrow(allData), 0.8*nrow(allData))
train <- allData[index,]
test <- allData[-index,]
model1<- glm(Like~., data=train, family=binomial)
summary(model1)
library(ROCR)
pred.model1.train<- predict(model1, type="response")
pred <- prediction(pred.model1.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo, data=train, family=binomial)
summary(model2)
pred.model2.train<- predict(model2, type="response")
pred <- prediction(pred.model2.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pcut<- mean(allData$Like)
class.model2<- (pred.prob.model2>pcut)*1
confMatr <- table(test$Like, class.model2, dnn = c("True", "Predicted"))
prop.table(confMatr, 1)
mean(test$Like!=class.model2)
library(rpart)
library(rpart.plot)
tree1 <- rpart(formula = Like~., data=train, method="class")
prp(tree1, digits = 4, extra = 1)
test.pred.tree1 <- predict(tree1, newdata=test, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree1, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
mean(test$Like!=test.pred.tree1)
summary(model2)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo, data=train, family=binomial)
summary(model2)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveliness, data=train, family=binomial)
summary(model2)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveliness, data=train, family=binomial)
summary(model2)
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveness, data=train, family=binomial)
summary(model2)
pred.model2.train<- predict(model2, type="response")
pred <- prediction(pred.model2.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pcut<- mean(allData$Like)
class.model2<- (pred.prob.model2>pcut)*1
confMatr <- table(test$Like, class.model2, dnn = c("True", "Predicted"))
prop.table(confMatr, 1)
mean(test$Like!=class.model2)
library(rpart)
library(rpart.plot)
tree1 <- rpart(formula = Like~., data=train, method="class")
prp(tree1, digits = 4, extra = 1)
test.pred.tree1 <- predict(tree1, newdata=test, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree1, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
mean(test$Like!=test.pred.tree1)
summary(model2)
tree.large <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(tree.large, digits = 4, extra = 1)
plotcp(tree.large)
tree2 <- rpart(formula = Like~., data=train, method="class", cp=0.017)
prp(tree2, digits = 4, extra = 1)
test.pred.tree2 <- predict(tree2, newdata=test, type="class")
# Confusion Matrix
confMatr <- table(test$Like, test.pred.tree2, dnn = c("True", "Pred"))
prop.table(confMatr, 1)
# Misclassification Rate -- (FP+FN)/Total
MR2 <- mean(test$Like!=test.pred.tree2)
MR2
# AUC
tree2.test.prob.rpart<- predict(tree2,test, type="prob")
pred = prediction(tree2.test.prob.rpart[,2], test$Like)
AUC2 <- slot(performance(pred, "auc"), "y.values")[[1]]
AUC2
index<- sample(1:nrow(allData), 0.8*nrow(allData))
train <- allData[index,]
test <- allData[-index,]
model1<- glm(Like~., data=train, family=binomial)
summary(model1)
library(ROCR)
pred.model1.train<- predict(model1, type="response")
pred <- prediction(pred.model1.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
model2<- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveness, data=train, family=binomial)
summary(model2)
pred.model2.train<- predict(model2, type="response")
pred <- prediction(pred.model2.train, train$Like)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
pred.prob.model2 <- predict(model2, newdata = test, type="response")
pcut<- mean(allData$Like)
class.model2<- (pred.prob.model2>pcut)*1
confMatr <- table(test$Like, class.model2, dnn = c("True", "Predicted"))
prop.table(confMatr, 1)
mean(test$Like!=class.model2)
# read in data, got from python using Spotipi with Spotify Web API
allData <- read.csv("~/Desktop/Spotify/featuresData.csv")
badData <- read.csv("~/Desktop/Spotify/badFeatures.csv")
goodData <- read.csv("~/Desktop/Spotify/goodFeatures.csv")
names(allData)
drop <- c("X", "id", "uri", "track_href", "analysis_url", "type")
allData = allData[,!(names(allData) %in% drop)]
badData = badData[,!(names(badData) %in% drop)]
goodData = goodData[,!(names(goodData) %in% drop)]
names(allData)
#Initial Summary of data
sumstat_fun<- function(x){
c(Mean=mean(x, na.rm=T), Std=sd(x, na.rm = T),
Min=min(x, na.rm = T), Max=max(x, na.rm = T))
}
ap1 <-apply(badData, 2, sumstat_fun)
ap2 <- apply(goodData, 2, sumstat_fun)
plainModel <- glm(Like~., data=train, family=binomial)
library(ROCR)
index<- sample(1:nrow(allData), 0.8*nrow(allData))
train <- allData[index,]
test <- allData[-index,]
plainModel <- glm(Like~., data=train, family=binomial)
summary(plainModel)
pred.plainModel.train<- predict(plainModel, type="response")
pred.plainModel <- prediction(pred.plainModel.train, train$Like)
perf.plainModel <- performance(pred.plainModel, "tpr", "fpr")
plot(perf.plainModel, colorize=TRUE)
plainModelAUC <- unlist(slot(performance(pred.plainModel, "auc"), "y.values"))
summary(plainModel)
reducedLM <- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveness, data=train, family=binomial)
summary(reducedLM)
pred.reducedLM.train <- predict(reducedLM, type="response")
pred.reducedLM <- prediction(pred.reducedLM.train, train$Like)
perf.reducedLM <- performance(pred.reducedLM, "tpr", "fpr")
plot(perf.reducedLM, colorize=TRUE)
#Get the AUC
reducedLM.AUC <- unlist(slot(performance(pred.reducedLM, "auc"), "y.values"))
pred.prob.reducedLM <- predict(reducedLM, newdata = test, type="response")
naivepcut<- mean(allData$Like)
class.reducedLM2<- (pred.prob.reducedLM>naivepcut)*1
reducedLM.confMatr <- table(test$Like, class.reducedLM2, dnn = c("True", "Predicted"))
prop.table(reducedLM.confMatr, 1)
reducedLM.MR <- mean(test$Like!=class.reducedLM2)
plainTree <- rpart(formula = Like~., data=train, method="class")
prp(plainTree, digits = 4, extra = 1)
tree.large <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(tree.large, digits = 4, extra = 1)
plotcp(tree.large)
tree2 <- rpart(formula = Like~., data=train, method="class", cp=0.017)
prp(tree2, digits = 4, extra = 1)
library(ROCR)
library(rpart)
library(rpart.plot)
set.seed(2020)
index<- sample(1:nrow(allData), 0.8*nrow(allData))
train <- allData[index,]
test <- allData[-index,]
plainModel <- glm(Like~., data=train, family=binomial)
summary(plainModel)
reducedLM <- glm(Like~.-loudness -acousticness -time_signature -duration_ms -instrumentalness -tempo -speechiness -liveness, data=train, family=binomial)
summary(reducedLM)
plainTree <- rpart(formula = Like~., data=train, method="class")
prp(plainTree, digits = 4, extra = 1)
largeTree <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(largeTree, digits = 4, extra = 1)
plotcp(largeTree)
setCPTree <- rpart(formula = Like~., data=train, method="class", cp=0.033)
prp(setCPTree, digits = 4, extra = 1)
largeTree <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(largeTree, digits = 4, extra = 1)
plotcp(largeTree)
setCPTree <- rpart(formula = Like~., data=train, method="class", cp=0.033)
prp(setCPTree, digits = 4, extra = 1)
largeTree <- rpart(formula = Like~., data=train, method="class", cp=0.001)
prp(largeTree, digits = 4, extra = 1)
plotcp(largeTree)
setCPTree <- rpart(formula = Like~., data=train, method="class", cp=0.016)
prp(setCPTree, digits = 4, extra = 1)
largeTreeAsym <- rpart(formula = DLRSN~., data=train, method = "class", parms = list(loss=matrix(c(0,10,1,0), nrow = 2)), cp=.001)
largeTreeAsym <- rpart(formula = Like~., data=train, method = "class", parms = list(loss=matrix(c(0,10,1,0), nrow = 2)), cp=.001)
prp(largeTreeAsym, digits = 4, extra = 1)
plotcp(largeTreeAsym)
setCPTreeAsym <- rpart(formula = Like~., data=train, method="class", cp=0.0025)
prp(setCPTreeAsym, digits = 4, extra = 1)
largeTreeAsym <- rpart(formula = Like~., data=train, method = "class", parms = list(loss=matrix(c(0,10,1,0), nrow = 2)), cp=.001)
prp(largeTreeAsym, digits = 4, extra = 1)
plotcp(largeTreeAsym)
setCPTreeAsym <- rpart(formula = Like~., data=train, method="class", cp=0.0045)
prp(setCPTreeAsym, digits = 4, extra = 1)
train.Bag <- bagging(as.factor(default)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
library(randomForest)
library(gbm)
library(parallel)
library(xgboost)
train.Bag <- bagging(as.factor(default)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
library(rpart)
library(xgboost)
train.Bag <- bagging(as.factor(default)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
library(randomForest)
library(gbm)
library(parallel)
library(rpart)
library(xgboost)
train.Bag <- bagging(as.factor(default)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
library(randomForest)
library(gbm)
library(parallel)
library(ipred)
library(rpart)
library(xgboost)
train.Bag <- bagging(as.factor(default)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
train.Bag <- bagging(as.factor(Like)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
predprob.test.bag<- predict(train.Bag, newdata = test, type="prob")[,2]        # This predicts the probabilistic prediction instead of classification. This is used to analyze/determine pcut values
pred.test.bag <- prediction(predprob.test.bag, test$Like)
perf.test.bag <- performance(pred.test.bag, "tpr", "fpr")
plot(perf.test.bag, colorize=TRUE)
#Get the AUC
train.Bag.AUC <- unlist(slot(performance(pred, "auc"), "y.values"))
pred.test.bag <- prediction(predprob.test.bag, test$Like)
perf.test.bag <- performance(pred.test.bag, "tpr", "fpr")
plot(perf.test.bag, colorize=TRUE)
#Get the AUC
train.Bag.AUC <- unlist(slot(performance(pred.test.bag, "auc"), "y.values"))
train.Bag.AUC
predclass.test.bag <- predict(train.Bag, newdata = test, type="class")
train.Bag.ConfMatr <- table(test$Like, predclass.test.bag, dnn = c("True", "Pred"))
train.Bag.RateTable <- prop.table(train.Bag.ConfMatr, 1)
train.Bag.RateTable
MR <- mean(predclass.test.bag != test$Like)
FPR <- train.Bag.RateTable[1,2]
FNR <- train.Bag.RateTable[2,1]
MR
train.Bag <- bagging(as.factor(Like)~., data = train, nbagg=100,
method="class", parms=list(loss=matrix(c(0,5,1,0), nrow=2)))
predprob.test.bag<- predict(train.Bag, newdata = test, type="prob")[,2]        # This predicts the probabilistic prediction instead of classification. This is used to analyze/determine pcut values
pred.test.bag <- prediction(predprob.test.bag, test$Like)
perf.test.bag <- performance(pred.test.bag, "tpr", "fpr")
plot(perf.test.bag, colorize=TRUE)
#Get the AUC
train.Bag.AUC <- unlist(slot(performance(pred.test.bag, "auc"), "y.values"))
predclass.test.bag <- predict(train.Bag, newdata = test, type="class")
train.Bag.ConfMatr <- table(test$Like, predclass.test.bag, dnn = c("True", "Pred"))
train.Bag.RateTable <- prop.table(train.Bag.ConfMatr, 1)
train.Bag.MR <- mean(predclass.test.bag != test$Like)
train.Bag.FPR <- train.Bag.RateTable[1,2]
train.Bag.FNR <- train.Bag.RateTable[2,1]
train.Bag.AUC
train.rf<- randomForest(as.factor(Like)~., data = train, cutoff=c(3/4,1/4)) # classwt does not change the results; this cutoff would make the majority vote requirement for 0 to be 3/4 instead of .5 as before. This encourages a false positive over a false negative
train.rf
train.rf<- randomForest(as.factor(Like)~., data = train, cutoff=c(1/4,3/4)) # classwt does not change the results; this cutoff would make the majority vote requirement for 0 to be 3/4 instead of .5 as before. This encourages a false positive over a false negative
train.rf<- randomForest(as.factor(Like)~., data = train, cutoff=c(1/4,3/4)) # classwt does not change the results; this cutoff would make the majority vote requirement for 0 to be 3/4 instead of .5 as before. This encourages a false positive over a false negative
train.rf
train.rf<- randomForest(as.factor(Like)~., data = train, cutoff=c(1/8,7/8)) # classwt does not change the results; this cutoff would make the majority vote requirement for 0 to be 3/4 instead of .5 as before. This encourages a false positive over a false negative
train.rf
train.rf<- randomForest(as.factor(Like)~., data = train, cutoff=c(7/8,1/8)) # classwt does not change the results; this cutoff would make the majority vote requirement for 0 to be 3/4 instead of .5 as before. This encourages a false positive over a false negative
train.rf
plot(credit.rf, lwd=rep(2, 3))
plot(train.rf, lwd=rep(2, 3))
legend("right", legend = c("OOB Error", "FPR", "FNR"), lwd=rep(2, 3), lty = c(1,2,3), col = c("black", "red", "green"))
plot(train.rf, lwd=rep(2, 3))
legend("right", legend = c("OOB Error", "FPR", "FNR"), lwd=rep(2, 3), lty = c(1,2,3), col = c("black", "red", "green"))
plot(train.rf, lwd=rep(2, 3))
legend("right", legend = c("OOB Error", "FPR", "FNR"), lwd=rep(2, 3), lty = c(1,2,3), col = c("black", "red", "green"))
# Prediction
train.rf.pred<- predict(train.rf, newdata=test, type = "prob")[,2]
rf.pred <- prediction(train.rf.pred, test$Like)
rf.perf <- performance(rf.pred, "tpr", "fpr")
plot(rf.perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(rf.pred, "auc"), "y.values"))   #AUC is pretty similar to bagging
rf.class.test<- predict(train.rf, newdata=test, type = "class")
table(test$Like, rf.class.test, dnn = c("True", "Pred"))
credit.rf.class.test<- (credit.rf.pred>mean(credit.train$default))*1     #without this, it will default to symmetric
rf.class.test<- (rf.pred>mean(train$Like))*1     #without this, it will default to symmetric
rf.class.test<- (rf.pred>mean(allData$Like))*1     #without this, it will default to symmetric
rf.class.test<- (train.rf.pred>mean(allData$Like))*1     #without this, it will default to symmetric
table(test$Like, rf.class.test, dnn = c("True", "Pred"))
